{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluate-vgg-imagenet template version 3.0.0**\n",
    "\n",
    "Please let me now of any ideas to improve this template. **I recommend to not remove the version number and to check for new versions of this template at https://github.com/jonasrauber/tensorflow-imagenet when running into problems or starting a new notebook**. See comments to specify paths, increase data io performance, adjust batch size, control logging or specify the variables to train. VGG19 is just an example that can be easily replaced with other networks.\n",
    "\n",
    "This template is for TensorFlow 1.0 or higher. Therefore, it is currently necessary to checkout the update-slim branch in the tensorflow/models repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:56:31.866230",
     "start_time": "2017-02-21T14:56:31.855987"
    }
   },
   "outputs": [],
   "source": [
    "# specify the path to your local clone of https://github.com/tensorflow/models,\n",
    "# which is used to load ImageNet and apply VGG preprocessing\n",
    "TENSORFLOW_MODELS = 'PLEASE SPECIFY'\n",
    "\n",
    "# specify the path of the ImageNet tfrecords files\n",
    "IMAGENET_DATA = '/gpfs01/bethge/data/imagenet'\n",
    "\n",
    "# specify the path to your initial checkpoint to restore pretrained models, e.g. VGG19\n",
    "INITIAL_CHECKPOINT = '/gpfs01/bethge/data/tf-model-checkpoints/vgg_19.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:56:35.622371",
     "start_time": "2017-02-21T14:56:31.869230"
    }
   },
   "outputs": [],
   "source": [
    "# ipython configuration\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# standard library\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# third-party packages\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "import tqdm\n",
    "\n",
    "# local modules\n",
    "IMPORT_PATHS = [os.path.join(TENSORFLOW_MODELS, 'slim')]\n",
    "sys.path.extend(set(IMPORT_PATHS) - set(sys.path))\n",
    "from datasets import imagenet\n",
    "from preprocessing import vgg_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:56:35.692461",
     "start_time": "2017-02-21T14:56:35.626811"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is NOT the same as the vgg_19 in tensorflow/models or tf.contrib.slim\n",
    "def vgg_19(inputs,\n",
    "           is_training,\n",
    "           dropout_keep_prob=0.5,\n",
    "           scope='vgg_19',\n",
    "           reuse=False):\n",
    "    \"\"\"VGG19 implementation using fully-connected layers\n",
    "    \n",
    "    This is an implementation of VGG19 using fully-connected layers rather\n",
    "    than 1x1 convolutions, because fully-connected layers are slightly faster.\n",
    "    To evaluate images of arbitrary size, replace this with a\n",
    "    fully-convolutional network definition.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'vgg_19', [inputs], reuse=reuse) as sc:\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d]):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "            net = slim.flatten(net)\n",
    "            net = slim.fully_connected(net, 4096, scope='fc6')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n",
    "            net = slim.fully_connected(net, 4096, scope='fc7')\n",
    "            net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout7')\n",
    "            net = slim.fully_connected(net, 1000, activation_fn=None, normalizer_fn=None, scope='fc8')\n",
    "            return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:56:35.743155",
     "start_time": "2017-02-21T14:56:35.697741"
    }
   },
   "outputs": [],
   "source": [
    "def get_validation_data():\n",
    "    with tf.device('/cpu:0'):\n",
    "        dataset = imagenet.get_split('validation', IMAGENET_DATA)\n",
    "        provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            dataset,\n",
    "            num_readers=1, # controls the speed at which data is loaded\n",
    "            shuffle=False,\n",
    "            common_queue_capacity=256,\n",
    "            common_queue_min=128)\n",
    "        image, label = provider.get(['image', 'label'])\n",
    "        \n",
    "        # preprocess the image\n",
    "        image = vgg_preprocessing.preprocess_for_eval(\n",
    "            image,\n",
    "            224,\n",
    "            224,\n",
    "            resize_side=256)\n",
    "        \n",
    "        # preprocess the label\n",
    "        label = tf.subtract(label, 1) # 1..1000 to 0..999\n",
    "\n",
    "    images, labels = tf.train.batch(\n",
    "        [image, label],\n",
    "        batch_size=64, # specify the batch size here\n",
    "        num_threads=16, # controls the speed at which images are preprocessed\n",
    "        capacity=256)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:56:37.557701",
     "start_time": "2017-02-21T14:56:35.745900"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    # load the data\n",
    "    images, labels = get_validation_data()\n",
    "    \n",
    "    # apply the model\n",
    "    predictions = vgg_19(images, is_training=False)\n",
    "    \n",
    "    # define the metrics\n",
    "    top5_accuracy, top5_accuracy_update = tf.metrics.recall_at_k(labels, predictions, k=5)\n",
    "    \n",
    "    # initialize local variables used by recall_at_k\n",
    "    init_op = tf.group(\n",
    "        tf.local_variables_initializer(),\n",
    "        tf.global_variables_initializer())\n",
    "    \n",
    "    # restore model variables\n",
    "    restorer = tf.train.Saver(slim.get_model_variables(), reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-21T14:56:52.575045",
     "start_time": "2017-02-21T14:56:37.560600"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(init_op)\n",
    "    restorer.restore(sess, INITIAL_CHECKPOINT)\n",
    "    with slim.queues.QueueRunners(sess):\n",
    "        n_batches = 10\n",
    "        for _ in tqdm.trange(n_batches):\n",
    "            sess.run(top5_accuracy_update)\n",
    "            \n",
    "        print(sess.run(top5_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
